{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Vectorizing data"
      ],
      "metadata": {
        "id": "AamRT520S5LL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uploading the .db file"
      ],
      "metadata": {
        "id": "X1VAblRLXFlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "w-VxZ8yKh88v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downloading the driver file"
      ],
      "metadata": {
        "id": "5D_0odFyXV2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.34.0/sqlite-jdbc-3.34.0.jar\n",
        "!wget https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/spark-nlp_2.12/5.1.4/spark-nlp_2.12-5.1.4.jar\n",
        "!wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.900/aws-java-sdk-bundle-1.11.900.jar"
      ],
      "metadata": {
        "id": "eyTOJP2cXVqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing the necessary libraries"
      ],
      "metadata": {
        "id": "SPuxZF5cXitv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3drZoKVy_MfH"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!pip install spark-nlp==5.1.4\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import of necessary libraries"
      ],
      "metadata": {
        "id": "KMTEVPpBXvFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Word2Vec\n",
        "from sparknlp.base import DocumentAssembler, Finisher\n",
        "from sparknlp.annotator import LemmatizerModel, Tokenizer as SparkNLPTokenizer, StopWordsCleaner"
      ],
      "metadata": {
        "id": "zAq_GcS0Jnc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating a spark session"
      ],
      "metadata": {
        "id": "IALYXfonX4qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        " .config('spark.jars', './*') \\\n",
        " .getOrCreate()"
      ],
      "metadata": {
        "id": "lvQ_VZs7UMR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the post table in dataframe"
      ],
      "metadata": {
        "id": "FmVT74mCYB4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('jdbc') \\\n",
        " .options(driver='org.sqlite.JDBC', dbtable='posts',\n",
        "       url='jdbc:sqlite:telegram_data.db') \\\n",
        " .load()"
      ],
      "metadata": {
        "id": "r_RoreItUViU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Removing extra characters"
      ],
      "metadata": {
        "id": "4rz9BQf_wAvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('post_text', F.regexp_replace(df.post_text, '[^\\\\wа-яА-Я\\\\s]', ''))"
      ],
      "metadata": {
        "id": "39ZOOJEJwAAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Collecting documents from the text of posts"
      ],
      "metadata": {
        "id": "d5nbTLrnlTm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        " .setInputCol(\"post_text\") \\\n",
        " .setOutputCol(\"document\")"
      ],
      "metadata": {
        "id": "gQOST7znkgDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting documents to tokens"
      ],
      "metadata": {
        "id": "dT6WaTtZlWGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_nlp_tokenizer = SparkNLPTokenizer() \\\n",
        " .setInputCols([\"document\"]) \\\n",
        " .setOutputCol(\"token\")"
      ],
      "metadata": {
        "id": "eYjQXjUUk5cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Removing stop words"
      ],
      "metadata": {
        "id": "4sRVBs3klwRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = StopWordsCleaner.pretrained(\"stopwords_ru\", \"ru\") \\\n",
        " .setInputCols([\"token\"]) \\\n",
        " .setOutputCol(\"cleanTokens\")"
      ],
      "metadata": {
        "id": "TxXgR-bEk7LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lemmatizing tokens"
      ],
      "metadata": {
        "id": "iXZtcW9Wl6R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = LemmatizerModel.pretrained(\"lemma\", \"ru\") \\\n",
        " .setInputCols([\"cleanTokens\"]) \\\n",
        " .setOutputCol(\"lemma\")"
      ],
      "metadata": {
        "id": "b2MKWOPWk8de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Collect final results into an array"
      ],
      "metadata": {
        "id": "n5WnDDM9mUSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finisher = Finisher() \\\n",
        " .setInputCols([\"lemma\"]) \\\n",
        " .setIncludeMetadata(False)"
      ],
      "metadata": {
        "id": "Yoj_W4WOk9pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a pipeline for data processing and apply it to the data"
      ],
      "metadata": {
        "id": "rEsQcgPHmiFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[documentAssembler, spark_nlp_tokenizer, stop_words, lemmatizer, finisher])\n",
        "result = pipeline.fit(df).transform(df)"
      ],
      "metadata": {
        "id": "0YKv0RF6k_1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set up the Word2Vec model for vectorization and apply it to data"
      ],
      "metadata": {
        "id": "5XfMyFiqPCxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2Vec = Word2Vec(vectorSize=100, minCount=5, inputCol=\"finished_lemma\", outputCol=\"vector\")\n",
        "result = word2Vec.fit(result).transform(result)s"
      ],
      "metadata": {
        "id": "t5PUDt_GPCgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion results"
      ],
      "metadata": {
        "id": "iJUISjM7oKDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for row in result.select('vector').collect():\n",
        " print(row)"
      ],
      "metadata": {
        "id": "FxLhOcd1lBxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Open Spark UI"
      ],
      "metadata": {
        "id": "hcJyYAXYS9Wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Downloading Spark to the environment"
      ],
      "metadata": {
        "id": "Z7n5MXL6S3_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar -xvzf spark-3.5.0-bin-hadoop3.tgz > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_k3Zp1iPhTI",
        "outputId": "a4f5236d-db81-496f-ebf7-094f556b8adc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-24 17:07:38--  https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.95.219, 135.181.214.104, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.95.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400395283 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.0-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.0-bin-had 100%[===================>] 381.85M  28.4MB/s    in 14s     \n",
            "\n",
            "2023-11-24 17:07:53 (27.2 MB/s) - ‘spark-3.5.0-bin-hadoop3.tgz’ saved [400395283/400395283]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Setting up the environment"
      ],
      "metadata": {
        "id": "BC8vvkB-Tn0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\""
      ],
      "metadata": {
        "id": "FojD_0vTP3Pk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Install and run findspark"
      ],
      "metadata": {
        "id": "Q0wcEajtTtga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "kuVlhltnP_x2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Import pySpark"
      ],
      "metadata": {
        "id": "kZXNCkFvT3u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark import SparkContext"
      ],
      "metadata": {
        "id": "7WizTYZuTwhU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Initate Spark environment"
      ],
      "metadata": {
        "id": "9e5q8jmbT_IG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf().setAppName(\"DataFrame\").set('spark.ui.port', '4050')\n",
        "spark = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "7bLPtdW5QEWh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Grabing the ngrok application and assigning token key"
      ],
      "metadata": {
        "id": "g-wmWTyIUGcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xvzf ngrok-v3-stable-linux-amd64.tgz\n",
        "!./ngrok authtoken 2YMQOAF2om5z0WgkKb2Fm84ZEQd_3uuoD3Bfc5K6DNMcjcg5U"
      ],
      "metadata": {
        "id": "5tRJunugQHO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Starting an agent and connecting Spark UI to ngrok"
      ],
      "metadata": {
        "id": "t7jekadkUewD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "metadata": {
        "id": "LlBjUXwTQRft"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Showing a direct link to the Spark UI"
      ],
      "metadata": {
        "id": "65vADNgwUlfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | grep -Po 'public_url\":\"(?=https)\\K[^\"]*'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGu1y_1nQT67",
        "outputId": "7d7e57fd-e455-483c-b580-7f18940137ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://ef0d-35-188-101-219.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}